{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubspaceModel(nn.Module):\n",
    "    def __init__(self, \n",
    "        dim: int,           # d\n",
    "        num_basis: int      # q\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.U = nn.Parameter(torch.empty((dim, num_basis)))    # size(d, q)\n",
    "        nn.init.orthogonal_(self.U)\n",
    "        self.L = nn.Parameter(torch.FloatTensor([3 * i for i in range(num_basis, 0, -1)])) #\n",
    "        self.mu = nn.Parameter(torch.zeros(dim)) # size(d, 1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.U.mm(self.L * z) + self.mu\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "        in_channels: int, \n",
    "        out_channels: int, \n",
    "        kernel_size: int = 3, \n",
    "        stride: int = 1, \n",
    "        padding: int = 1, \n",
    "        padding_mode: str = \"zeros\", \n",
    "        # groups: int = 1, \n",
    "        # # bias: bool = True, \n",
    "        transposed: bool = False, \n",
    "        # normalization: str = None, \n",
    "        activation: bool = True, \n",
    "        pre_activate: bool = False\n",
    "    ) -> None:\n",
    "        if transposed:\n",
    "            conv = partial(nn.ConvTranspose2d, output_padding=stride - 1)\n",
    "            padding_mode = \"zeros\"\n",
    "        else:\n",
    "            conv = nn.Conv2d\n",
    "        \n",
    "        layers = [conv(\n",
    "                    in_channels, \n",
    "                    out_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    padding=padding,\n",
    "                    padding_mode=padding_mode\n",
    "                    )]\n",
    "        if activation:\n",
    "            if pre_activate:\n",
    "                layers.insert(0, nn.LeakyReLU())\n",
    "            else:\n",
    "                layers.append(nn.LeakyReLU())\n",
    "        super().__init__(*layers)\n",
    "\n",
    "class EigenBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "        width: int, \n",
    "        height: int, \n",
    "        in_channels: int, \n",
    "        out_channels: int, \n",
    "        num_basis: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.subspacelayer = SubspaceModel(\n",
    "            dim=width * height * in_channels, \n",
    "            num_basis=num_basis\n",
    "        )\n",
    "        self.subspace_conv1 = ConvLayer( # output size H\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            transposed=True,\n",
    "            activation=False\n",
    "        )\n",
    "        self.subspace_conv2 = ConvLayer( # output size 2H\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            transposed=True,\n",
    "            activation=False\n",
    "        )\n",
    "        self.feature_conv1 = ConvLayer( # output size 2H\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            transposed=True,\n",
    "            pre_activate=True\n",
    "        )\n",
    "        self.feature_conv2 = ConvLayer( # output size H\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            transposed=True,\n",
    "            pre_activate=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, h):\n",
    "        phi = self.subspacelayer(z).view(h.shape)\n",
    "        h = self.feature_conv1(self.subspace_conv1(phi) + h)\n",
    "        h = self.feature_conv2(self.subspace_conv2(phi) + h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,\n",
    "        size: int = 256,\n",
    "        num_basis: int = 6,\n",
    "        noise_dim: int = 512, # generator input (\\epsilon) dim\n",
    "        base_channels: int = 16,\n",
    "        max_channels: int = 512\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        assert (size & (size-1) == 0) and size != 0, \"image size should be a power of 2.\"\n",
    "\n",
    "        self.noise_dim = noise_dim\n",
    "        self.num_basis = num_basis\n",
    "        self.num_blocks = int(math.log(size, 2) - 2) # size = 4 * (2**num_blocks)\n",
    "\n",
    "        def getChannelsNumber(block_idx: int):\n",
    "            return min(max_channels, base_channels * (2**(self.num_blocks - block_idx)))\n",
    "        \n",
    "        self.fc_layer = nn.Linear(self.noise_dim, 4 * 4 * getChannelsNumber(0))\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(self.num_blocks):\n",
    "            self.blocks.append(\n",
    "                EigenBlock(\n",
    "                    width=4 * 2**i,\n",
    "                    height=4 * 2**i,\n",
    "                    in_channels=getChannelsNumber(i),\n",
    "                    out_channels=getChannelsNumber(i + 1),\n",
    "                    num_basis=self.num_basis\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            ConvLayer(\n",
    "                base_channels,\n",
    "                3,\n",
    "                kernel_size=7,\n",
    "                stride=1,\n",
    "                padding=3, # keep the size same\n",
    "                pre_activate=True\n",
    "            ),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def getDevice(self):\n",
    "        return self.fc_layer.weight.device\n",
    "\n",
    "    def sampleLatentVariables(self, batch: int):\n",
    "        device = self.getDevice()\n",
    "        epsilon_samples = torch.randn(batch, self.noise_dim, device= device)\n",
    "        z_samples = torch.randn(batch, self.num_blocks, self.num_basis, device=device) # sample z of each block together\n",
    "        return epsilon_samples, z_samples\n",
    "    \n",
    "    def forward(self, latent_variables):\n",
    "        epsilon_samples, z_samples = latent_variables\n",
    "\n",
    "        output = self.fc_layer(epsilon_samples).view(len(epsilon_samples), -1, 4, 4)\n",
    "        for block, z_sample in zip(self.blocks, torch.permute(z_samples, (1, 0, 2))):\n",
    "            output = block(z_sample, output)\n",
    "        \n",
    "        return self.output_layer(output)\n",
    "\n",
    "    def regularizeUOrthogonal(self):\n",
    "        reg = []\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, SubspaceModel):\n",
    "                UUT = layer.U.matmul(layer.U.transpose())\n",
    "                reg.append(\n",
    "                    torch.mean((UUT - torch.eye(UUT.shape([0]), device=UUT.device))**2)\n",
    "                    )\n",
    "        return sum(reg) / len(reg)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,\n",
    "        size: int = 256,\n",
    "        base_channels: int = 16,\n",
    "        max_channels: int = 512    \n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        blocks = [\n",
    "            ConvLayer(\n",
    "                3,\n",
    "                base_channels,\n",
    "                kernel_size=7,\n",
    "                stride=1,\n",
    "                padding=3 # keep the same size\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        num_channels = base_channels\n",
    "        for _ in range(int(math.log(size, 2) - 2)):\n",
    "            next_num_channels = min(max_channels, num_channels * 2)\n",
    "            blocks += [\n",
    "                ConvLayer(num_channels, num_channels, kernel_size=3, stride=1), # same size\n",
    "                ConvLayer(num_channels, next_num_channels, kernel_size=3, stride=2) # downsample to H/2\n",
    "                ]\n",
    "            num_channels = next_num_channels\n",
    "        blocks.append(ConvLayer(num_channels, num_channels, kernel_size=3, stride=1))\n",
    "\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Flatten(), #\n",
    "            nn.Linear(4 * 4 * num_channels, num_channels),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(num_channels, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.blocks(input)\n",
    "        return self.output_layer(output)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('PINNs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5651587e625deb4fe86239a16248888bca0533ef8a7ebc9a31b892346d8b21d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
